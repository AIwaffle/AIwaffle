{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as tfs\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport os\nfrom PIL import Image\nimport random\nprint(torch.__version__)\nprint(cv2.__version__)","execution_count":10,"outputs":[{"output_type":"stream","text":"1.3.0\n4.1.2\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"数据预处理参考 https://zhuanlan.zhihu.com/p/32506912"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_images(root=r'../input/pascal-voc-2012/VOC2012', train=True):\n    filename = root + '/ImageSets/Segmentation/' + ('train.txt' if train else 'val.txt')\n    with open(filename, 'r') as f:\n        images = f.read().split()\n    data = [os.path.join(root, 'JPEGImages', i + '.jpg') for i in images]\n    label = [os.path.join(root, 'SegmentationClass', i + '.png') for i in images]\n    return data, label\n\ndef rand_crop(data, label, height, width):\n    '''\n    data is PIL.Image object\n    label is PIL.Image object\n    '''\n    w, h = data.size\n    if w == width and h == height:\n        i, j = 0, 0\n    else:\n        i = random.randint(0, h - height)\n        j = random.randint(0, w - width)\n    \n    data = tfs.functional.crop(data, i, j, height, width)\n    label = tfs.functional.crop(label, i, j, height, width)\n    return data, label","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = ['background','aeroplane','bicycle','bird','boat',\n           'bottle','bus','car','cat','chair','cow','diningtable',\n           'dog','horse','motorbike','person','potted plant',\n           'sheep','sofa','train','tv/monitor']\n\n# RGB color for each class\ncolormap = [[0,0,0],[128,0,0],[0,128,0], [128,128,0], [0,0,128],\n            [128,0,128],[0,128,128],[128,128,128],[64,0,0],[192,0,0],\n            [64,128,0],[192,128,0],[64,0,128],[192,0,128],\n            [64,128,128],[192,128,128],[0,64,0],[128,64,0],\n            [0,192,0],[128,192,0],[0,64,128]]\n\nlen(classes), len(colormap)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(21, 21)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm2lbl = np.zeros(256**3)\nfor i,cm in enumerate(colormap):\n    cm2lbl[(cm[0]*256+cm[1])*256+cm[2]] = i\n\ndef image2label(im):\n    data = np.array(im, dtype='int32')\n    idx = (data[:, :, 0] * 256 + data[:, :, 1]) * 256 + data[:, :, 2]\n    return np.array(cm2lbl[idx], dtype='int64')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_im = Image.open('../input/pascal-voc-2012/VOC2012/SegmentationClass/2007_000033.png').convert('RGB')\nlabel = image2label(label_im)\nlabel[150:160, 240:250]","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def img_transforms(im, label, crop_size):\n    im, label = rand_crop(im, label, crop_size[0], crop_size[1])\n    im_tfs = tfs.Compose([\n        tfs.ToTensor(),\n        tfs.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    im = im_tfs(im)\n    label = image2label(label)\n    label = torch.from_numpy(label)\n    return im, label","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VOCSegDataset(torch.utils.data.Dataset):\n    '''\n    voc dataset\n    '''\n    def __init__(self, train, crop_size, transforms):\n        self.crop_size = crop_size\n        self.transforms = transforms\n        data_list, label_list = read_images(train=train)\n        self.data_list = self._filter(data_list)\n        self.label_list = self._filter(label_list)\n        print('Read ' + str(len(self.data_list)) + ' images')\n        \n    def _filter(self, images):\n        return [im for im in images if (Image.open(im).size[1] >= self.crop_size[0] and \n                                        Image.open(im).size[0] >= self.crop_size[1])]\n        \n    def __getitem__(self, idx):\n        img = self.data_list[idx]\n        label = self.label_list[idx]\n        img = Image.open(img)\n        label = Image.open(label).convert('RGB')\n        img, label = self.transforms(img, label, self.crop_size)\n        return img, label\n    \n    def __len__(self):\n        return len(self.data_list)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CityscapesDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, root_dir):\n        self.data_dir = glob.glob(root_dir + r'*.jpg')\n        \n    def __len__(self):\n        return len(self.data_dir)\n    \n    def __getitem__(self, idx):\n        '''\n        Args: scalar index\n        Returns: input_img, ground_truth\n        '''\n        img = cv2.imread(self.data_dir[idx])\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img, gt = torch.tensor(img[:, 0:256, :], dtype=torch.float), torch.tensor(img[:, 256:512, :], dtype=torch.float)\n        img, gt = img.permute(2, 0, 1), gt.permute(2, 0, 1)\n        torchvision.transforms.Normalize([73.27711, 82.75635, 72.56262], [44.13835, 45.155933, 44.69525], inplace=True)(img)\n        return img, gt","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# train = CityscapesDataset(r'../input/cityscapes-image-pairs/cityscapes_data/train/')\n# val = CityscapesDataset(r'../input/cityscapes-image-pairs/cityscapes_data/val/')\ninput_shape = (320, 480)\ntrainSet = VOCSegDataset(True, input_shape, img_transforms)\ntestSet = VOCSegDataset(False, input_shape, img_transforms)","execution_count":31,"outputs":[{"output_type":"stream","text":"Read 1114 images\nRead 1078 images\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = torch.utils.data.DataLoader(trainSet, 64, shuffle=True, num_workers=4)\nvalid_data = torch.utils.data.DataLoader(testSet, 128, num_workers=4)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate mean and std\n# mean, std = torch.zeros(3), torch.zeros(3)\n# for img, gt in train:\n#     mean += torch.mean(img, dim=(1, 2))\n#     std += torch.std(img, dim=(1, 2))\n# mean = (mean / len(train)).numpy()\n# std = (std / len(train)).numpy()\n# print(mean, std)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.hub.list('pytorch/vision', force_reload=True)","execution_count":15,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/pytorch/vision/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n","name":"stderr"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"['alexnet',\n 'deeplabv3_resnet101',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'fcn_resnet101',\n 'googlenet',\n 'inception_v3',\n 'mobilenet_v2',\n 'resnet101',\n 'resnet152',\n 'resnet18',\n 'resnet34',\n 'resnet50',\n 'resnext101_32x8d',\n 'resnext50_32x4d',\n 'shufflenet_v2_x0_5',\n 'shufflenet_v2_x1_0',\n 'squeezenet1_0',\n 'squeezenet1_1',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'wide_resnet101_2',\n 'wide_resnet50_2']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_net = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\nnum_classes = len(classes)\n\nclass FCN(nn.Module):\n    def __init__(self, num_classes):\n        super(FCN, self).__init__()\n\n        self.stage1 = nn.Sequential(*list(pretrained_net.children())[:-4])\n        self.stage2 = list(pretrained_net.children())[-4]\n        self.stage3 = list(pretrained_net.children())[-3]\n        \n        self.scores1 = nn.Conv2d(512, num_classes, 1)\n        self.scores2 = nn.Conv2d(256, num_classes, 1)\n        self.scores3 = nn.Conv2d(128, num_classes, 1)\n        \n        self.upsample_8x = nn.ConvTranspose2d(num_classes, num_classes, 16, 8, 4, bias=False)\n        \n        self.upsample_4x = nn.ConvTranspose2d(num_classes, num_classes, 4, 2, 1, bias=False)\n        \n        self.upsample_2x = nn.ConvTranspose2d(num_classes, num_classes, 4, 2, 1, bias=False)\n\n        \n    def forward(self, x):\n        x = self.stage1(x)\n        s1 = x # 1/8\n        \n        x = self.stage2(x)\n        s2 = x # 1/16\n        \n        x = self.stage3(x)\n        s3 = x # 1/32\n        \n        s3 = self.scores1(s3)\n        s3 = self.upsample_2x(s3)\n        s2 = self.scores2(s2)\n        s2 = s2 + s3\n        \n        s1 = self.scores3(s1)\n        s2 = self.upsample_4x(s2)\n        s = s1 + s2\n\n        s = self.upsample_8x(s2)\n        return s","execution_count":16,"outputs":[{"output_type":"stream","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_master\nDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n100%|██████████| 83.3M/83.3M [00:01<00:00, 57.1MB/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = FCN(num_classes)\nmodel.cuda()\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":18,"outputs":[{"output_type":"stream","text":"FCN(\n  (stage1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (stage3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (scores1): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n  (scores2): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n  (scores3): Conv2d(128, 21, kernel_size=(1, 1), stride=(1, 1))\n  (upsample_8x): ConvTranspose2d(21, 21, kernel_size=(16, 16), stride=(8, 8), padding=(4, 4), bias=False)\n  (upsample_4x): ConvTranspose2d(21, 21, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (upsample_2x): ConvTranspose2d(21, 21, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(100):\n    model.train()\n    \n    running_loss = 0.0\n    \n    for img, gt in train_data:\n        img = img.cuda()\n        gt = gt.cuda()\n        out = model(img)\n        loss = loss_fn(out, gt)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    print(f'epoch: {epoch}/100, loss: {running_loss}')\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}